{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Course:** Randomized Algorithms by Dr. Zarei\n",
        "\n",
        "**Homework:** HW2\n",
        "\n",
        "**Name:** Mohammad Mohammadi\n",
        "\n",
        "**Student ID:** 402208592"
      ],
      "metadata": {
        "id": "9JbKLJNkfqlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.2.1. [DARA_Hor]\n",
        "Let U be a finite set, |U| = r ¬∑ m for a positive integer r.\n",
        "Estimate the number of functions h : U ‚Üí T that satisfy the property (3.1)."
      ],
      "metadata": {
        "id": "TNS-hA4yyNoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "Let's assume a common scenario in hashing or load balancing where each function h should map elements of U to elements of T in a way that each output in T is hit an approximately equal number of times (a balanced mapping). Assume ‚à£T‚à£=m. This assumption is common in exercises related to hash functions where the goal is uniform distribution across the target set.\n",
        "\n",
        "**Estimation:**\n",
        "\n",
        "1.  Total Number of Functions: Without any restriction, the total number of functions from U to T is $m^{‚à£U‚à£}=m^{r‚ãÖm}$, since each element in U can be mapped to any of the m elements in T.\n",
        "2.  Balanced Functions:\n",
        "    *   We want each element in T to be hit r times (since ‚à£U‚à£=r‚ãÖm).\n",
        "    *   The number of ways to choose r elements out of r‚ãÖm for one particular element in T (while ensuring the same count for others) follows a combinatorial approach.\n",
        "3.  Using Combinatorics: To compute the number of ways to assign each t‚ààT exactly r pre-images in U, we:\n",
        "    *   Choose r elements from U to map to the first element in T,\n",
        "    *   Then choose r elements from the remaining U for the second element in T,\n",
        "    *   Continue this until all elements in T are assigned r elements from U.\n",
        "\n",
        "This can be calculated as:\n",
        "$$\\binom{r \\cdot m}{r} \\binom{r \\cdot (m-1)}{r} \\ldots \\binom{r}{r}$$\n",
        "Simplifying, this is equivalent to:\n",
        "$$\\frac{(r \\cdot m)!}{(r!)^m}$$\n",
        "\n",
        "**Conclusion**\n",
        "Assuming h maps each element to each target with equal frequency, the estimated number of such functions h is $\\frac{(r \\cdot m)!}{(r!)^m}$. Based on examples I've seen online, this is a typical analysis in scenarios requiring uniform distribution.\n"
      ],
      "metadata": {
        "id": "brpU2KqhyiB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.2.4. [DARA_Hor]\n",
        "\n",
        "Let S be a randomly chosen group of persons. How large has S to be in order to assure:\n",
        "\n",
        "Prob(two persons from S were born on the same day) ‚â• 1/2 ?\n",
        "\n",
        "To answer this question, consider the indicator variable X defined by\n",
        "\n",
        "X(S) = {1: if there are two persons in S with the same birthday| 0: else}\n",
        "\n",
        "and assume that the birthdays are uniformly distributed over the year.\n",
        "\n",
        "Estimate E[X] as a function of |S|."
      ],
      "metadata": {
        "id": "j1ZYGmIO2iYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "First, let's calculate P(X(S)=1), the probability that at least two persons in S have the same birthday.\n",
        "\n",
        "1.  Complement Rule: Instead of directly calculating P(X(S)=1), it's easier to calculate its complement, P(X(S)=0), which is the probability that no two persons in S share the same birthday.\n",
        "2.  Calculation of P(X(S)=0):\n",
        "    *   The first person can have any birthday, which happens with a probability of 1 (365/365).\n",
        "    *   The second person must have a different birthday than the first, which can happen with a probability of (364/365).\n",
        "    *   The third person must have a different birthday than the first two, which can happen with a probability of (363/365), and so on.\n",
        "    *   Therefore, for ‚à£S‚à£=n, the probability that no one shares a birthday is:\n",
        "\n",
        "\n",
        "$$ P(X(S) = 0) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\ldots \\times \\frac{365-n+1}{365}$$\n",
        "\n",
        "Using the product notation:\n",
        "\n",
        "$$P(X(S) = 0) = \\prod_{k=0}^{n-1} \\left(\\frac{365-k}{365}\\right)$$\n",
        "\n",
        "\n",
        "Since X(S) is an indicator variable, E[X]=P(X(S)=1), which is:\n",
        "\n",
        "$$E[X] = P(X(S) = 1) = 1 - P(X(S) = 0)$$\n",
        "\n",
        "\n",
        "To find the smallest n for which $E[X]‚â• 1/2$, we calculate when $P(X(S)=0)‚â§ 1/2$.\n",
        "\n",
        "From empirical calculations and known results, this threshold is reached at n=23, as this is the smallest number for which the probability of no shared birthday is less than or equal to 0.5.\n",
        "\n",
        "\n",
        "\n",
        "Hence, to assure that the probability of at least two persons from a randomly chosen group S having the same birthday is at least 1/2, the group must have at least 23 persons. This results in E[X] being at least 0.5 for ‚à£S‚à£=23, fulfilling the condition given in the exercise.\n",
        "\n",
        "\n",
        "### Refs:\n",
        "1.  Wikipedia page for the Birthday Problem:\n",
        "https://en.wikipedia.org/wiki/Birthday_problem\n",
        "2.  BetterExplained: Understanding the Birthday Paradox\n",
        "https://betterexplained.com/articles/understanding-the-birthday-paradox/\n"
      ],
      "metadata": {
        "id": "SxlYb8bA3WfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.3.10. [DARA_Hor]\n",
        "\n",
        "Let us consider the set\n",
        "\n",
        "M = {h : U ‚Üí T | h fulfills (3.1)}.\n",
        "\n",
        "Is M a universal set of hash functions?"
      ],
      "metadata": {
        "id": "i4JKcuCo6E4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "Definition of a Universal Set of Hash Functions:\n",
        "\n",
        "A set of hash functions is considered universal if for every pair of distinct keys $k_1$ and $k_2$ from the universe of keys U, the probability of a collision (i.e., $h(k_1) = h(k_2)$) under a randomly chosen hash function h from the set is low, typically 1/‚à£T‚à£ where ‚à£T‚à£ is the size of the target set.\n",
        "\n",
        "Adversary Foiling:\n",
        "\n",
        "The essence of \"foiling the adversary\" is about designing a set of hash functions that remain effective even when an adversary knows the functions and attempts to provide inputs that would typically lead to worst-case performances (like clustering of hash values, leading to inefficiencies).\n",
        "\n",
        "Assessment:\n",
        "\n",
        "If M indeed meets these criteria ‚Äî particularly if it ensures a low probability of collision across distinct keys and maintains efficiency across a range of adversarial inputs ‚Äî then M could potentially be considered a universal set. However, true universality in the strictest sense would depend on the precise mathematical properties M guarantees, which should include a uniform distribution of hash values and independence between the choices of hash values for different keys.\n",
        "\n",
        "In Conclusion:\n",
        "\n",
        "While M might exhibit characteristics whit similar nature or characteristic to those of a universal set of hash functions, whether it qualifies as universal depends on the strict adherence to the mathematical definition involving low collision probabilities and performance under adversarial conditions. We can conjecture based on typical goals in such design scenarios but can't conclusively declare M universal without detailed properties."
      ],
      "metadata": {
        "id": "xHYdj2eSAXLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4.2.1. [DARA_Hor]\n",
        "\n",
        "Apply the amplification method in order to enable a randomized test of x ‚àà U for larger sets U. Estimate the maximal possible cardinality of U for which the achieved error probability still approaches 0 with growing n when the communication complexity is\n",
        "\n",
        "(i) in O(log n ¬∑ log log n),\n",
        "\n",
        "(ii) in O ((log n)^d) for any constant d ‚àà IN,\n",
        "\n",
        "(iii) polylogarithmic."
      ],
      "metadata": {
        "id": "pG4bmCBICsey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "**(i) Communication Complexity in $ùëÇ\n",
        "(\n",
        "log\n",
        "‚Å°\n",
        "ùëõ\n",
        "‚ãÖ\n",
        "log\n",
        "‚Å°\n",
        "log\n",
        "‚Å°\n",
        "ùëõ\n",
        ")$**\n",
        "\n",
        "For a communication complexity of $ùëÇ\n",
        "(\n",
        "log\n",
        "‚Å°\n",
        "ùëõ\n",
        "‚ãÖ\n",
        "log\n",
        "‚Å°\n",
        "log\n",
        "‚Å°\n",
        "ùëõ\n",
        ")$, the size of U is limited by how much information can be transmitted under this bound. To estimate the maximum cardinality of U, consider that the logarithmic terms suggest that the size of U can grow exponentially with n as long as the exponent itself is restricted to $logn‚ãÖloglogn$. This means:\n",
        "\n",
        "$$|U| \\approx 2^{\\log n \\cdot \\log \\log n} = n^{\\log \\log n}$$\n",
        "\n",
        "This estimation allows the error probability to approach zero with repeated testing n times, assuming that errors decrease exponentially with the number of tests.\n",
        "\n",
        "**(ii) Communication Complexity in $\n",
        "ùëÇ\n",
        "(\n",
        "(\n",
        "log\n",
        "‚Å°\n",
        "ùëõ\n",
        ")^ùëë\n",
        ")$**\n",
        "\n",
        "When the complexity is bounded by $O((logn)^d)$, for some constant d, the exponential growth in U's size can be more significant compared to the previous case. This time:\n",
        "\n",
        "$$|U| \\approx 2^{(\\log n)^d}$$\n",
        "\n",
        "Here, n can be extremely large, depending on d. The function $(logn)^d$grows much slower than n but faster than $logn‚ãÖloglogn$, allowing for a larger set U while still ensuring that the error probability approaches zero as n increases.\n",
        "\n",
        "\n",
        "**(iii) Polylogarithmic Communication Complexity**\n",
        "\n",
        "A polylogarithmic complexity, often noted as $poly(logn)$, implies a complexity that can be expressed as a polynomial function of $logn$. This gives:\n",
        "\n",
        "$$|U| \\approx 2^{\\text{poly}(\\log n)}$$\n",
        "\n",
        "This notation still encapsulates a very large growth potential for U but within a manageable computational framework. The exact nature of the polynomial affects the specific growth rate, but in general, this would allow for a very large U compared to typical logarithmic or linear growth rates.\n"
      ],
      "metadata": {
        "id": "Ge3ttWxDEYzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4.2.5. [DARA_Hor]\n",
        "\n",
        "Transform the protocol PDisj to a Las Vegas protocol. How large is its expected communication complexity?"
      ],
      "metadata": {
        "id": "7wckCXmUGRS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "To transform a Monte Carlo protocol into a Las Vegas protocol, one key requirement is to ensure that the protocol always produces a correct result with the possibility of a variable runtime, rather than having a fixed runtime with a chance of error as in Monte Carlo protocols.\n",
        "\n",
        "1.  Run the Monte Carlo PDisj Repeatedly: We start by running the original Monte Carlo version of PDisj. If this version can err by falsely claiming that the sets are not disjoint (or any type of error relevant to the original PDisj), then we need to handle this in the Las Vegas version.\n",
        "2.  Verification Step: After each run of the Monte Carlo PDisj, we introduce a verification step. If the Monte Carlo PDisj concludes that the sets are disjoint, we verify this result by checking a subset or utilizing a deterministic approach that guarantees correctness for that specific case.\n",
        "3.  Repeat if Necessary: If the verification step fails (i.e., if the Monte Carlo result is found to be incorrect), the algorithm repeats both the Monte Carlo test and the verification until the correct result is confirmed.\n",
        "4.  Termination: The Las Vegas protocol will only terminate when a correct result is achieved and verified, ensuring that it always produces the correct output, albeit with a potentially variable runtime.\n",
        "\n",
        "\n",
        "**Expected Communication Complexity**\n",
        "\n",
        "The expected communication complexity of this Las Vegas version of PDisj will depend heavily on:\n",
        "*   The error probability of the original Monte Carlo PDisj.\n",
        "*   The communication cost of each Monte Carlo run.\n",
        "*   The communication cost of the verification step.\n",
        "\n",
        "Typically, the Las Vegas version has an expected communication complexity calculated as follows:\n",
        "\n",
        "$$\\text{Expected Communication Complexity} = \\sum_{k=1}^{\\infty} (\\text{Communication per run} \\times P(\\text{Error})^{k-1})$$\n",
        "\n",
        "Where $P(Error)$ is the probability that the Monte Carlo algorithm errs. If $P(Error)$ is relatively low and the communication cost per run is moderate, the series converges quickly, and the expected communication complexity remains reasonable.\n",
        "\n",
        "For protocols like PDisj, where the Monte Carlo version might have an error probability $P(Error)$ that could be adjusted (e.g., by adjusting the parameters of the Monte Carlo test), the expected complexity can be quite efficient.\n",
        "\n",
        "This transformation leverages the ability to repeat the probabilistic test multiple times to drive the error probability down exponentially with each additional run, typical of Las Vegas algorithms where runtime (or in this case, communication) can increase to ensure correctness."
      ],
      "metadata": {
        "id": "wtSHAXh7R-Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4.5.17. [DARA_Hor]\n",
        "\n",
        "Consider the communication protocol R presented in Section 1.2 for the comparison of two binary strings (of the contents of two databases). Let p be a prime and let $a = a_1a_2...a_n, a_i ‚àà {0,1} for i = 1, ..., n$, be a binary string. Consider the polynomial $P_a$ of a singe variable x defined over $Z_p$ as follows.\n",
        "\n",
        "$$P_a(x) = \\sum_{i=1}^{n} a_i x^{i-1}$$\n",
        "\n",
        "Apply the idea of algorithm AQP on order to design a 1MC protocol for the comparison of two n bit strings $a = a_1a_2 ...a_n and b = b_1b_2 ...b_n$. What effect does the choice of p have on the error probability and on the communication complexity? Compare the results with the complexity and the success probability of the protocol R from Section 1.2."
      ],
      "metadata": {
        "id": "ZUOD9RAPTtZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "The idea is to evaluate both polynomials at a randomly chosen point and compare the results.\n",
        "\n",
        "1.  **Polynomial Representation:**\n",
        "Define polynomials for both strings over $\\mathbb{Z}_ùëù$ (where p is a prime number) as:\n",
        "\n",
        "$$P_a(x) = \\sum_{i=1}^n a_i x^{i-1}, \\quad P_b(x) = \\sum_{i=1}^n b_i x^{i-1}$$\n",
        "\n",
        "Here, x is a variable and $ùëé_ùëñ\n",
        ",\n",
        "ùëè_ùëñ\n",
        "‚àà\n",
        "\\{\n",
        "0\n",
        ",\n",
        "1\n",
        "\\}$ are the bits of the binary strings.\n",
        "\n",
        "2.  **Random Evaluation:**\n",
        "Randomly choose a value x from $\\mathbb{Z}_ùëù$ and compute $\n",
        "ùëÉ_ùëé\n",
        "(\n",
        "ùë•\n",
        ")$ and $\n",
        "ùëÉ_ùëè\n",
        "(\n",
        "ùë•\n",
        ")$.\n",
        "\n",
        "3.  **Comparison:** If $\n",
        "ùëÉ_ùëé\n",
        "(\n",
        "ùë•\n",
        ") =\n",
        "ùëÉ_ùëè\n",
        "(\n",
        "ùë•\n",
        ")$, conclude $a=b$; otherwise, conclude $ùëé‚â†ùëè$.\n",
        "\n",
        "**Impact of the Choice of p on the Protocol**\n",
        "\n",
        "*   Error Probability: If $ùëé‚â†ùëè$ $P_a(x) - P_b(x)$ is a non-zero polynomial of degree at most n‚àí1. The number of roots of a non-zero polynomial over a field can be at most the degree of the polynomial. Hence, the probability that a random x leads to a false positive (where $P_a(x) = P_b(x)$ but $ùëé‚â†ùëè$) is at most $(n‚àí1)/p$. Therefore, a larger p decreases the error probability.\n",
        "\n",
        "*   Communication Complexity: Communicating the value of $P_a(x)$ or $P_b(x)$ requires $log_2(p)$ bits since the values are elements of $\\mathbb{Z}_p$. Thus, larger values of p increase the communication cost.\n",
        "\n",
        "**Comparison with Protocol R**\n",
        "\n",
        "Protocol R uses a set of randomly chosen prime moduli and communicates results of modular arithmetic. It achieves high efficiency with relatively low communication complexity and a very low error probability, owing to the use of multiple primes and combining results from independent trials.\n",
        "\n",
        "*   Error Probability: The error probability in Protocol R can be extremely low, as calculated using the prime number theorem and properties of bad primes. For large n, Protocol R ensures an error probability that falls well below typical physical error rates in computational systems.\n",
        "*   Communication Complexity: Protocol R communicates only a few integers (the results of modular reductions), making it very efficient in terms of the number of bits sent, especially when compared to protocols requiring larger bit communications.\n",
        "\n",
        "In comparison, the 1MC protocol using polynomial evaluation may have a simpler implementation but could potentially suffer from higher communication costs and error rates without careful choice of the prime p and possibly more sophisticated error reduction techniques such as repeating the protocol with different values of x and taking majority votes or using error-correcting methods.\n"
      ],
      "metadata": {
        "id": "h5dUYnpMVyFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5.2.2. [DARA_Hor]\n",
        "\n",
        "Modify the algorithm CONTRACTION in the following way. Instead of choosing an edge at random, choose two vertices x and y randomly and join them into one vertex. Construct multigraphs of n vertices, for which the probability that the modified algorithm finds a minimal cut is exponentially small in n.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Algorithm CONTRACTION:\n",
        "\n",
        "Input: A connected multigraph G = (V, E, c)\n",
        "Step 1: Set label (v) := {v} for every vertex v ‚àà V.\n",
        "Step 2:\n",
        "    while G has more than two vertices do\n",
        "        begin\n",
        "            choose an edge e = {x, y} ‚àà E(G);\n",
        "            G := Contract(G, e);\n",
        "            Set label(z) := label(x) ‚à™ label(y)\n",
        "            for the new vertex z = ver(x, y);\n",
        "        end\n",
        "Step3:\n",
        "    if G = ({u, v}, E(G)) for a multiset E(G) then\n",
        "        output ‚Äú(label (u) , label (v))‚Äù and ‚Äúcost = |E(G)|‚Äù\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "z3y_WydwaM18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "1.  **Initialization:** As in the original algorithm, start with each vertex v in the graph G=(V,E,c) labeled individually.\n",
        "2.  **Contraction Step:**\n",
        "    *   Instead of choosing an edge e={x,y} randomly from E, choose two vertices x and y randomly from V.\n",
        "    *   If there exists an edge e between x and y, proceed to merge x and y into a new vertex z. If no edge exists, either skip the step or choose a different pair until an edge is found (the how to of the choice here can affect the algorithm's performance and determine how efficientlyy it can run).\n",
        "    *   Update the graph by replacing x and y with z, combining the incident edges while preserving multiedges and self-loops.\n",
        "    *   Update labels to reflect the merger: label(z) := label(x) ‚à™ label(y)\n",
        "3.  **Termination:**\n",
        "    *   Continue the contraction until only two vertices remain.\n",
        "    *   Output the labels of the two remaining vertices and the number of edges (multiedges counted separately) between them as the cut-cost.\n",
        "\n",
        "\n",
        "**Algorithm Analysis**\n",
        "\n",
        "*   **Random Selection of Vertices:** The key difference is that vertices are selected randomly without regard to whether they are connected by an edge. This can increase the number of steps needed to reduce the graph to two vertices, as non-adjacent vertices might be selected.\n",
        "*   **Probability of Finding Minimal Cut:** The probability that the modified algorithm finds a minimal cut is lower than in the original algorithm. In the original, each edge contraction step directly contributes to potentially separating the graph into two parts, maintaining connectivity until the very end. By randomly choosing vertices, the chance of separating a minimal cut without prematurely isolating a vertex or merging across a minimal cut decreases.\n",
        "*   **Exponential Decrease in Success Probability:** For graphs where minimal cuts consist of few edges between large clusters of vertices, the modified approach has an exponentially smaller probability of finding these cuts. It's much more likely to join vertices across different cuts before isolating these crucial edges, especially in dense graphs or graphs with complex connectivity.\n",
        "\n",
        "\n",
        "**Constructing Graphs for Small Success Probability**\n",
        "To construct a graph where the modified algorithm performs poorly (i.e., the probability of finding a minimal cut is exponentially small), consider a graph structure where:\n",
        "\n",
        "*   The graph has a high degree of connectivity (dense graph).\n",
        "*   Minimal cuts are not evident or are distributed such that random vertex selection frequently bypasses these cuts.\n",
        "*   Graphs with symmetrical structures or multiple equivalent minimal cuts spread across the structure, causing random selections to often merge vertices from different cuts.\n",
        "\n",
        "**IN Conclusion:**\n",
        "\n",
        "This modification generally reduces the effectiveness of the CONTRACTION algorithm in finding minimal cuts due to the lack of targeted edge selection, potentially increasing the number of steps required and reducing the likelihood of isolating minimal cuts before the graph is reduced to two vertices. For applications where finding a minimum cut is crucial, this modified algorithm would be less preferred by default."
      ],
      "metadata": {
        "id": "LGQodACtbQne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5.2.8. [DARA_Hor] [OPTIONAL - EXTRA POINT]\n",
        "\n",
        "Analyze the success probability and the time complexity of the versions of the algorithm REPTREE, for which one takes the following size reduction between two splits of the computation:\n",
        "\n",
        "(i) from $l$ to $\\left\\lfloor \\frac{l}{2} \\right\\rfloor$\n",
        "\n",
        "(ii) from $l$ to $\\sqrt{l}$\n",
        "\n",
        "(iii) from $l$ to $\\frac{l}{\\log_2 l}$\n",
        "\n",
        "(iv) from $l$ to $l - \\sqrt{l}$\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Algorithm REPTREE(G)\n",
        "\n",
        "Input: A multigraph G=(V,E,c), ‚à£V‚à£=n, n‚ààN, n‚â•3.\n",
        "\n",
        "Procedure:\n",
        "\n",
        "if n ‚â§ 6 then\n",
        "    compute a minimal cut deterministically\n",
        "else\n",
        "    begin\n",
        "    h := \\left\\lceil 1 + \\frac{n}{\\sqrt{2}} \\right\\rceil;\n",
        "    Perform two independent runs of CONTRACTION in order to get two multigraphs G/F1 and G/F2 of size h;\n",
        "    REPTREE(G/F1);\n",
        "    REPTREE(G/F2)\n",
        "    end\n",
        "output the smaller of the two cuts computed by REPTREE(G/F1) and REPTREE(G/F2)\n",
        "\n",
        "```\n",
        "\n",
        "**Theorem 5.2.5:** The algorithm REPTREE works in time $O(n^2‚ãÖlogn)$ and finds a minimal cut with a probability of at least $\\frac{1}{Œ©(log_2 n)}$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A3bvVFW2fgBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer"
      ],
      "metadata": {
        "id": "0x6DfnyM02pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (i)\n",
        "\n",
        "**Time Complexity:**\n",
        "*   This division leads to a typical binary recursion where the problem size halves at each level. The depth of recursion is therefore O(logl).\n",
        "*   If each level of the recursion takes time proportional to the current size (say O(l) per level), the total time complexity using the Master Theorem or a simple recursive sum is O(l).\n",
        "\n",
        "**Success Probability:**\n",
        "*   Assuming that each recursive call must succeed for the overall success (which is typical in algorithms like Karger's min-cut where global properties depend on local correctness), and if each has a constant independent success probability p, the overall success probability is $p^{logl} = l^{logp}$. For p small, this diminishes with increasing l.\n",
        "\n"
      ],
      "metadata": {
        "id": "lSZbfEgr04cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (ii)\n",
        "\n",
        "**Time Complexity:**\n",
        "*   Here, the recursion depth increases since $\\sqrt{l}$ shrinks the problem size slower than halving. The recursion depth is approximately O(loglogl).\n",
        "*   The time complexity, assuming O(l) work per level, results in a total complexity potentially greater than O(l) due to multiple layers operating on still substantial problem sizes.\n",
        "\n",
        "**Success Probability:**\n",
        "*   As in the binary case, but now the probability degrades slower, potentially resulting in higher reliability if p is sufficiently large.\n",
        "\n"
      ],
      "metadata": {
        "id": "0NMXXK5m1fr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iii)\n",
        "\n",
        "**Time Complexity:**\n",
        "*   The problem size is reduced slower than in the previous cases, implying a deeper recursion, potentially around O(logl).\n",
        "*   If the work at each level is O(l), the total complexity can be analyzed as similar or slightly worse than O(llogl).\n",
        "\n",
        "**Success Probability:**\n",
        "*   Better than in the case of a halving reduction due to a lesser reduction in problem size each step, leading to more opportunities for error correction or success consolidation.\n",
        "\n"
      ],
      "metadata": {
        "id": "bmQhFMmT1_6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iv)\n",
        "\n",
        "**Time Complexity:**\n",
        "*   This reduction is very slow, leading to a very high number of recursive calls, potentially $O(\\sqrt{l})$.\n",
        "*   Given O(l) work at each level, this might result in a high overall complexity.\n",
        "\n",
        "**Success Probability:**\n",
        "*   Due to many steps and a small reduction per step, this could be high if the failure rate per step is not too large.\n",
        "\n"
      ],
      "metadata": {
        "id": "1PeMKqe-2OWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In summry\n",
        "\n",
        "*   Fastest convergence in size reduction: (i) and (ii) are preferred.\n",
        "\n",
        "*   Balance between depth and error resilience: (iii) might offer a trade-off.\n",
        "\n",
        "*   Highest error resilience but possibly inefficient: (iv)."
      ],
      "metadata": {
        "id": "a2iwLmNX2f5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5.3.11. [DARA_Hor] [OPTIONAL - EXTRA POINT]\n",
        "\n",
        "Extend the algorithm SCH√ñNING for 4SAT. Observe that the lower bound on the probability of moving toward $Œ±^*$ in a local step decreases to 1/4 in this case. How many repetitions of random sampling followed by a local search are necessary to get a constant success probability?"
      ],
      "metadata": {
        "id": "JzbhKyINfqn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "When extending this algorithm to 4SAT, the probability of flipping a variable that makes the current assignment closer to a satisfying assignment $\\alpha^*$ decreases to 1/4. This change impacts the expected number of steps required before either finding a satisfying assignment or restarting.\n",
        "\n",
        "For 4SAT, the probability p of succeeding in a single run of local search is impacted by the lower probability 1/4 of making the correct variable flip at each step. The expected length L of the local search where a flip successfully moves closer to $\\alpha^*$ is given by the drift analysis, often simplified to around 4n steps, given the new probability 1/4 and assuming the worst case of n variables being incorrect.\n",
        "\n",
        "Sch√∂ning‚Äôs analysis provides that for 3SAT, the success probability p of a single run is approximately $\\left(\\frac{1}{3}\\right)^n \\cdot e^n$. Extending to 4SAT with similar analysis but using 1/4 gives us an approximate success probability per trial of:\n",
        "\n",
        "$$p ‚âà \\left(\\frac{1}{4}\\right)^n \\cdot e^n$$\n",
        "\n",
        "To ensure a constant success probability P, such as P=0.99, we use the formula for the probability of failure after T independent trials:\n",
        "\n",
        "\n",
        "$$1-P=(1-p)^T$$\n",
        "\n",
        "\n",
        "$$T = \\frac{\\log(1 - P)}{\\log(1 - p)}$$\n",
        "\n",
        "Inserting the approximate p for large n:\n",
        "\n",
        "$$T \\approx \\frac{\\log(0.01)}{\\log(1-(\\frac{1}{4^n} \\cdot e^n))} \\approx$$\n",
        "\n",
        "$$T \\approx \\frac{-\\log(0.01)}{\\frac{1}{4^n} \\cdot e^n}$$\n",
        "\n",
        "\n",
        "This calculation shows that T grows very fast with n, indicating a need for a large number of trials to achieve a high probability of finding a satisfying assignment, reflecting the inherent difficulty of 4SAT."
      ],
      "metadata": {
        "id": "KFmN9Pru3OGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5.4.16. [DARA_Hor] [OPTIONAL - EXTRA POINT]\n",
        "\n",
        "Modify the algorithm NQUAD in such a way that it always halts with a quadratic nonresidue (i.e., the answer \"?\" never appears). Analyze the expected running time of your modified NQUAD and prove that the probability of executing an infinite computation is 0."
      ],
      "metadata": {
        "id": "IO-0b-VzfqtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "To modify NQUAD to always halt with a quadratic nonresidue and prevent indefinite computation, consider the following:\n",
        "\n",
        "*   Instead of one random selection, repeatedly select random elements until a nonresidue is found.\n",
        "*   Limit the number of trials to ensure that the algorithm does not run indefinitely, with theoretical justification that a nonresidue will be found within these limits.\n",
        "\n",
        "\n",
        "**Mathematical Justification:**\n",
        "\n",
        "*   Given that approximately half of the elements of $F_p$ are nonresidues, the probability p of selecting a quadratic nonresidue in one trial is $\\frac{p-1}{2p} \\approx \\frac{1}{2}$.\n",
        "*   The probability of not finding a nonresidue in k independent trials is $(1-\\frac{1}{2})^k = \\frac{1}{2^k}$.\n",
        "*   To ensure that the algorithm almost surely finds a nonresidue, k can be chosen such that $\\frac{1}{2^k}$ is negligibly small, ensuring the probability of running indefinitely (i.e., never finding a nonresidue) approaches zero.\n",
        "\n",
        "\n",
        "Since each trial is independent and has a success probability of approximately 1/2, the expected number of trials until success is E(T)=1/p, where p=1/2, yielding E(T)=2. This represents an expected two trials per execution of the algorithm.\n",
        "\n",
        "1.  The expected number of trials needed to find a quadratic nonresidue is 2 (as calculated).\n",
        "2.  If each trial (including selection and verification) takes O(1) time, the expected running time of the modified NQUAD algorithm is O(1).\n",
        "3.  The probability of executing more than k trials decreases exponentially with k, and $lim_{ùëò‚Üí‚àû} \\frac{1}{2^k} = 0$, which means the probability of an infinite computation is zero."
      ],
      "metadata": {
        "id": "AOhJLi7D7Mks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7.1 [RA_Mot]\n",
        "\n",
        "\n",
        "In this problem we will see that Theorem 7.1 is actually just a special case of Theorem 7.2. In the setting of Theorem 7.1. construct a multivariate polynomial Q such that $Q \\equiv 0$ if and only if AB = C. and then apply Theorem 7.2 to derive result in Theorem 7.1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gEJCVzrmfvPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "**Defining Polynomial Q**\n",
        "\n",
        "Given matrices A, B and C all of size n√ón over a field F, we need to construct a multivariate polynomial Q such that Q‚â°0 if and only if AB=C.\n",
        "\n",
        "Let the entries of $a_{i,j}$ and $b_{i,j}$ and $c_{i,j}$, respectively. The polynomial Q is defined over the field F as follows:\n",
        "\n",
        "$$Q(x_1, \\ldots, x_{n^2}, y_1, \\ldots, y_{n^2}) = \\sum_{i=1}^n \\sum_{j=1}^n \\left( \\sum_{k=1}^n a_{ik}b_{kj} - c_{ij} \\right)^2$$\n",
        "\n",
        "Where $x_1, ..., x_{n^2}$ and $y_1, ..., y_{n^2}$ represent the entries of A and B respectively, rearranged into vectors. This polynomial Q is zero if and only if each individual product $\\sum_{k=1}^n a_{ik}b_{kj}$ equals $c_{i,j}$, hence AB=C.\n",
        "\n",
        "**From Theorem 7.2**\n",
        "\n",
        "Theorem 7.2 states that for a multivariate polynomial Q of total degree d, and a finite set S‚äÇF from which variables are chosen independently and uniformly, the probability that Q evaluates to zero is at most d/‚à£S‚à£.\n",
        "\n",
        "1.  Total Degree: In Q, the highest total degree of any term arises from terms like $(a_{ik}b_{kj})^2$ within the squares, contributing a degree of 2 from each matrix entry multiplication where $a_{ik}$ and $b_{kj}$ are both potentially non-zero. Given n such multiplications per entry in the sum for C, the degree is 2n.\n",
        "2.  Applying the Bound: If r values are selected from $\\{0,1\\}^n$ (binary vectors if we consider the simplest case of F), Theorem 7.2 gives a bound on the probability $Pr[ABr=Cr]‚â§ 2n/2^n$ where the polynomial evaluates to zero, matching the setting of Theorem 7.1 which discusses the case of $Pr[ABr=Cr]‚â§1/2$.\n",
        "\n",
        "**In conclusion**\n",
        "\n",
        "By linking the polynomial representation of the matrix equation $AB=C$ and applying Theorem 7.2, we can deduce the result of Theorem 7.1 as a specific application. This shows that the probability of a random binary vector r satisfying $ABr=Cr$ when $AB‚â†C$ is indeed bounded by 1/2.\n"
      ],
      "metadata": {
        "id": "ocTnV67ogu-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7.2 [RA_Mot]\n",
        "\n",
        "\n",
        "Two rooted trees T1 and T2 are said to be isomorphic if there exists a one- to-one onto mapping f from the vertices of T1 to those of T2 satisfying the following condition: for each internal vertex v of T1 with the children $V_1, ..., V_k$. the vertex f(v) has as children exactly the vertices $f(V_1), ..., f(V_k)$. Observe that no ordering is assumed on the children of any internal vertex. Devise an efficient randomized algorithm for testing the isomorphism of rooted trees and analyze its performance. (Hint: Associate a polynomial $P_v$ with each vertex v in a tree T. The polynomials are defined recursively. the base case being that the leaf vertices all have $P = X_0$. An internal vertex v of height h with the children $V_1, ..., V_k$ has its polynomial defined to be\n",
        "\n",
        "$$(X_h - P_{V_1})(X_h - P_{V_2})...(X_h - P_{V_k})$$\n",
        "\n",
        "Note that there is exactly one indeterminate for each level in the tree.\n",
        "\n",
        "Remark: There is a linear time deterministic algorithm for this problem based on a similar approach. R~fer to Aho. Hopcroft and Ullman [5].\n"
      ],
      "metadata": {
        "id": "RoNdED4WKarY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "1.  **Polynomial Assignment:**\n",
        "    *   Begin at the leaves of each tree and assign each leaf vertex a polynomial $P_v = X_0$, where $X_0$ is a constant polynomial (e.g., could be a specific indeterminate or value).\n",
        "    *   Recursively, compute the polynomial for each internal vertex based on its children's polynomials. If an internal vertex v has children $V_1, ..., V_k$, then define:\n",
        "\n",
        "$$P_v = (X_h - P_{V_1})(X_h - P_{V_2})...(X_h - P_{V_k})$$\n",
        "        \n",
        "where $X_h$ is a unique indeterminate associated with the height h of the vertex v in the tree.\n",
        "\n",
        "2.  **Randomization:**\n",
        "    *   Choose a random value for each $X_h$ from a sufficiently large set (or field) to minimize the probability of collision (i.e., different trees having the same polynomial representation by coincidence).\n",
        "\n",
        "3.  **Comparison:**\n",
        "    *   Evaluate the polynomials at the roots of both trees $T_1$ and $T_2$. If the evaluated polynomials are identical for the chosen values of $X_h$, the trees are likely isomorphic.\n",
        "    *   Repeat the comparison multiple times with different random values for the $X_h$ variables to reduce the probability of a false positive.\n",
        "\n",
        "**Performance And Efficiency:**\n",
        "\n",
        "**Correctness:**\n",
        "\n",
        "*   If the polynomials of the root vertices of both trees are identical for one set of random assignments of $X_h$, it strongly suggests the trees are isomorphic because the structure defined by the recursive polynomial generation ensures that any permutation of children (since no order is assumed) would still yield the same polynomial if the trees are indeed isomorphic.\n",
        "\n",
        "**Error Probability:**\n",
        "*   The error in this algorithm could occur if non-isomorphic trees yield the same polynomial due to a specific \"unlucky\" choice of random values for $X_h$. However, this probability can be made arbitrarily small by choosing the values from a large enough field or set.\n",
        "\n",
        "**Complexity:**\n",
        "*   The algorithm is efficient because the computation of each polynomial at each node involves only operations proportional to the degree of the node (number of children), and the recursion runs depth-first from the leaves to the root, thus processing each vertex exactly once.\n",
        "*   The overall complexity in terms of polynomial operations would generally be linear with respect to the number of vertices in the trees, assuming polynomial operations take constant time, which is a common assumption in theoretical analyses."
      ],
      "metadata": {
        "id": "XgZA7HMyMsY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7.4 [RA_Mot]\n",
        "\n",
        "Consider the problem of deciding whether two integer multisets $S_1$ and $S_2$ are identical in the sense that each integer occurs the same number of times in both sets. This problem can be solved by sorting the two sets in O(n log n) time. where n is the cardinality of the multisets. Suggest a way of representing this as a problem involving a verification of a polynomial identity, and thereby obtain an efficient randomized algorithm. Discuss the relative merits of the two algorithms. keeping in mind issues such as the model of computation and the size of the integers being operated upon. (See also Problem 6.20.)"
      ],
      "metadata": {
        "id": "TZ0o02TAa0WY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "**Polynomial Representation of Multisets**\n",
        "Each multiset $S_1$ and $S_2$ can be represented as a polynomial where each integer a in the multiset contributes a term $x^a$ to the polynomial, and the coefficient of $x^a$ represents the multiplicity of a in the multiset. Thus, for a multiset S, the polynomial $P_s(x)$ can be represented as:\n",
        "\n",
        "$$P_S(x) = \\sum_{a \\in S} c_a x^a$$\n",
        "\n",
        "where $c_a$ is the count of integer a in multiset S.\n",
        "\n",
        "**Randomized Algorithm Using Polynomial Identity Testing**\n",
        "1.  Polynomial Construction: Construct two polynomials $P_{s_1}(x)$ and $P_{s_2}(x)$ as described, one for each multiset.\n",
        "2.  Random Evaluation: Evaluate both polynomials at a randomly chosen value x=r. This value r should be chosen from a sufficiently large range to minimize the probability of collision (where different polynomials might yield the same value).\n",
        "3.  Comparison: Compare $P_{s_1}(r)$ and $P_{s_2}(r)$. If $P_{s_1}(r) = P_{s_2}(r)$ for several random choices of r, with high probability, the multisets $S_1$ and $S_2$ are identical. If $P_{s_1}(r) \\neq P_{s_2}(r)$ for any r, then the multisets are definitely not identical.\n",
        "\n",
        "**Compariing with Sorting-based Algorithm**\n",
        "*   Performance: Sorting each multiset takes O(nlogn) time, which is predictable and guaranteed. The polynomial identity test typically runs in linear time for construction and evaluation but requires random choice and possibly multiple evaluations to reduce error probability.\n",
        "*   Model of Computation: Sorting algorithms are straightforward and don't rely on probabilistic outcomes. They are preferable in environments where deterministic results are critical. Polynomial identity tests, however, are beneficial when quick, approximate results are acceptable, particularly in large-scale data environments.\n",
        "*   Integer Size: The polynomial method can handle very large integers efficiently as long as they can be processed within the chosen data structures. In contrast, sorting large integers might involve overhead depending on the sorting algorithm and machine architecture."
      ],
      "metadata": {
        "id": "Ng1CsG4ubVgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7.13 [RA_Mot]\n",
        "\n",
        "Consider the two-dimensional version of the pattern matching problem. The text is an n x n matrix X, and the pattern is an m x m matrix Y. A pattern match occurs if Y appears as a (contiguous) sub-matrix of X. To apply the randomized algorithm described above, we convert the matrix Y into an $m^2$-bit vector using the row-major format. The possible occurrences of Y in X are the $m^2$-bit vectors $X(j)$) obtained by taking all $(n - m +1)^2$ sub-matrices of X in a row-major form. It is clear that the earlier algorithm can now be applied to this scenario. Analyze the error probability in this case, and explain how the fingerprints of each $X(j)$) can be computed at a small incremental cost."
      ],
      "metadata": {
        "id": "dcY11wubdSLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "For the two-dimensional pattern matching:\n",
        "\n",
        "First lets make Matrix to Bit Vector: Convert the m√óm pattern matrix Y into an $m^2$ -bit vector using row-major order (traverse each row of the matrix from left to right and then proceed to the next row). Similarly, each m√óm sub-matrix of X is also converted into an $m^2$ -bit vector $X(j)$ in row-major format.\n",
        "\n",
        "Then we should compute Fingerprints:\n",
        "\n",
        "*   Compute a \"fingerprint\" for the bit vector of Y, and similarly for each potential $m^2$ -bit vector $X(j)$ derived from the sub-matrices of X.\n",
        "*   A common approach to fingerprinting is using a hash function designed to minimize collisions. A suitable choice could be a polynomial rolling hash function, which can effectively handle the string-like structure of the bit vectors.\n",
        "\n",
        "Computing Fingerprints incrementally on top of each other:\n",
        "*   For each $X(j)$ and subsequent $X(j+1)$ (the next possible sub-matrix in row-major traversal), compute the fingerprint based on the previous fingerprint rather than from scratch.\n",
        "*   For example, if moving from $X(j)$ to $X(j+1)$ involves shifting the sub-matrix window to the right by one column, remove the effect of the leftmost column of $X(j)$ and add the effect of the new column that comes into $X(j+1)$.\n",
        "*   This approach significantly reduces the computational cost because it only requires updating parts of the hash that change rather than recomputing it entirely for each sub-matrix.\n",
        "\n",
        "Error probability in respect to hash collisions:\n",
        "*   The probability of error in this context arises primarily from the possibility of hash collisions, where two different bit vectors (i.e., different sub-matrices) might yield the same hash value.\n",
        "*   If using a well-designed hash function and a sufficiently large hash space, the probability of collisions can be significantly reduced. However, it's often a trade-off between computational efficiency and collision risk.\n",
        "*   Generally, the error probability is inversely related to the size of the hash space. For cryptographic hash functions, this is typically quite low.\n"
      ],
      "metadata": {
        "id": "UCyF2Um6egYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 13.1 [RA_Mot]\n",
        "\n",
        "(Due to D.O. Sleator and R.E. Tarjan [379].) Show that the LRU algorithm for paging is k-competitive. What can you say about its competitiveness coefficient?"
      ],
      "metadata": {
        "id": "OJiQdeKUnFJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "To analyze the performance of LRU in the context of competitive analysis, we compare the cost incurred by the algorithm to the cost incurred by an optimal offline algorithm that knows all future requests.\n",
        "\n",
        "**Definition of k-Competitive**\n",
        "\n",
        "An online algorithm is k-competitive if there exists a constant c such that for every input sequence, the cost of the online algorithm is at most k times the cost of the optimal offline algorithm, plus c. Formally, for every input sequence œÉ, $$Cost_{ALG}(œÉ) ‚â§ k . Cost_{OPT}(œÉ) + e$$\n",
        "\n",
        "\n",
        "The LRU paging algorithm evicts the least recently used page when a page fault occurs and a new page needs to be loaded into a full memory. This strategy is based on the assumption that pages accessed recently will likely be accessed again soon.\n",
        "\n",
        "Ref:\n",
        "*   https://embedded.cs.uni-saarland.de/lectures/realtimesystems/cachePredictabilityCompetitivenessSensitivity.pdf\n",
        "*   https://embedded.cs.uni-saarland.de/lectures/realtimesystems15/blocklevelcompetitivenessAnnotated.pdf\n",
        "\n",
        "**Competitiveness of LRU:**\n",
        "1.  A page fault occurs under LRU when a requested page is not in the cache. LRU replaces the page that has not been used for the longest time. The optimal offline strategy (OPT), on the other hand, can foresee future requests and always evicts the page that will not be needed for the longest time in the future.\n",
        "2.  Research by Sleator and Tarjan on the competitiveness of paging algorithms established that LRU is k-competitive where k is the number of pages the algorithm can store. The cost considered here is the number of page faults.\n",
        "3.  Well what is k?\n",
        "    *   The competitiveness coefficient k for LRU comes from the analysis showing that in the worst-case scenario, LRU's performance will be no worse than k times the number of page faults of the optimal algorithm, plus a constant related to the cache size.\n",
        "    *   This result is derived from considering the number of times a page can be brought into and evicted from the cache between two accesses by the optimal algorithm.\n",
        "\n",
        "Now we can demonstrate that the LRU algorithm is k-competitive for paging:\n",
        "\n",
        "1.  LRU Algorithm and OPT:\n",
        "Assume the cache can hold k pages. The LRU algorithm evicts the least recently used page when a new page needs to be loaded into a full cache. In contrast, an optimal offline algorithm (OPT) can always evict the page that will not be needed for the longest time in the future.\n",
        "2.  Cost Analysis:\n",
        "    *   Each page fault under LRU incurs a cost of 1.\n",
        "    *   The competitive analysis focuses on comparing the cost incurred by LRU against the cost incurred by OPT over the same sequence of page requests.\n",
        "3.  The Potential Method:\n",
        "    *   Define a potential function Œ¶ that measures the \"distance\" between the state of the cache under LRU and the state of the cache under OPT.\n",
        "    *   The potential function could be defined as the number of pages that are different in the LRU cache compared to the OPT cache.\n",
        "    *   Changes in this potential function help to measure the additional cost that LRU might incur over OPT during transitions (page replacements).\n",
        "4.  Bounding the Costs:\n",
        "    *   When LRU incurs a page fault (cost = 1), if OPT does not incur a fault, the potential Œ¶ may increase by at most 1 (because one wrong page in LRU is replaced by a correct page).\n",
        "    *   If OPT also incurs a page fault, the potential Œ¶ might decrease, which can offset the cost of the fault for LRU.\n",
        "    *   The amortized cost of LRU, considering the change in potential, remains constant or decreases.\n",
        "5.  Competitive Ratio:\n",
        "    *   The competitive ratio k for LRU is derived by observing that in the worst case, every time LRU evicts a page, OPT could ideally retain all pages that will be used in the near future, and thus LRU's faults could seem misaligned with OPT's faults.\n",
        "    *   However, because of the way the potential function is defined and adjusted, the maximum ratio of the cost of LRU to OPT is bounded by k, the size of the cache. Thus, LRU is k-competitive.\n",
        "\n",
        "\n",
        "In overal, the competitive analysis relies on the insight that even though LRU does not know the future, it makes a locally optimal decision based on past usage. This heuristic, while not always matching the foresight of OPT, still guarantees that the cache's turnover is managed efficiently compared to the optimal method known only in hindsight.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cvTQMsr8ndPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 13.2 [RA_Mot]\n",
        "\n",
        "(Due to D.O. Sleator and R.E. Tarjan [379].) Show that the FIFO algorithm for paging is k-competitive. What can you say about its competitiveness coefficient?"
      ],
      "metadata": {
        "id": "zbxISNoNt7uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "To show that the FIFO algorithm for paging is k-competitive, we analyze its performance relative to an optimal offline algorithm (OPT) as we did in previous problem.\n",
        "\n",
        "1.  **FIFO Behavior:** FIFO manages its cache by evicting the oldest page (the first one that came in) when a new page needs to be loaded, and the cache is full.\n",
        "2.  **Competitive Ratio Determination:**\n",
        "    *   Consider any sequence of page requests.\n",
        "    *   Let the cost of serving these requests using FIFO be the number of page faults (when a requested page is not in the cache and must be loaded).\n",
        "    *   The OPT can potentially have the least number of page faults because it knows future requests and can always keep the most immediately necessary pages in the cache.\n",
        "3.  **Bounding the Costs:**\n",
        "    *   For every page fault experienced by FIFO, there must be k distinct pages requested since the last time the faulting page was in the cache (otherwise, it would not have been evicted).\n",
        "    *   OPT must replace at least one page during these k distinct requests; therefore, it incurs at least one fault every k requests in the worst case.\n",
        "4.  **Competitive Analysis:** Since OPT has a minimum of one fault for every k requests that cause a fault in FIFO, the maximum ratio of faults between FIFO and OPT is k. Therefore, FIFO is k-competitive.\n",
        "5.  **Competitiveness Coefficient:** The competitiveness coefficient k implies that the worst-case performance of FIFO is at most k times the number of faults of OPT, plus a possible additive constant that depends on the initial state of the caches and the specific sequence of page requests.\n"
      ],
      "metadata": {
        "id": "8ss72sWvuTU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 13.3 [RA_Mot]\n",
        "\n",
        "Show that the LFU algorithm does not achieve a bounded competitiveness coefficient."
      ],
      "metadata": {
        "id": "h_8TuHb5vfcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer\n",
        "\n",
        "1.  **LFU Behavior:** LFU evicts the page with the lowest frequency of access when a new page needs to be loaded into a full cache.\n",
        "2.  **COnsidering Adversarial Input Sequence:** Construct an input sequence where the least frequently used page changes frequently due to the introduction of new pages or slight variations in access frequencies. For example:\n",
        "    *   Start with a sequence of pages 1,2,‚Ä¶,k to fill the cache.\n",
        "    *   Continue with pages k+1,1,k+2,2,‚Ä¶ where each new page k+i is accessed once more than the previous least accessed page which just got evicted.\n",
        "3.  **OPT Strategy:** An optimal offline algorithm (OPT) can anticipate future requests and keep pages that will be accessed more frequently, leading to fewer evictions.\n",
        "4.  **Comparison of Costs:**\n",
        "    *   Under LFU, the page eviction might lead to subsequent faults when the evicted page is accessed again soon after.\n",
        "    *   OPT may incur significantly fewer faults by evicting pages that will not be accessed for the longest time ahead.\n",
        "5.  **Unbounded Competitive Ratio:** In the adversarial sequence, the number of page faults for LFU can be much higher compared to OPT. For instance, LFU might incur a fault nearly every time if every new access slightly shifts the frequency distribution, whereas OPT strategically keeps pages that will minimize future faults.\n",
        "\n",
        "So, due to LFU's strict adherence to frequency and inability to adapt to changes in access patterns that are not reflected in the long-term historical frequencies, it can perform significantly worse than OPT in certain scenarios. Thus, LFU does not have a bounded competitiveness coefficient; its performance relative to OPT can drop drastically based on specific patterns of access frequencies."
      ],
      "metadata": {
        "id": "pSwnuiHRvyvj"
      }
    }
  ]
}